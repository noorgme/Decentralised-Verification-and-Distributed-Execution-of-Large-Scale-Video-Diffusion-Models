{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Video Generation with Zeroscope\n",
    "\n",
    "This notebook demonstrates how to use the distributed coordinator to generate videos by splitting the latent space into chunks and processing them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pynvml\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: nvidia-ml-py, pynvml\n",
      "Successfully installed nvidia-ml-py-12.570.86 pynvml-12.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialise Coordinator\n",
    "\n",
    "Create a coordinator instance with our desired configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed_coordinator import DistributedCoordinator\n",
    "\n",
    "coordinator = DistributedCoordinator(\n",
    "    model_id=\"cerspense/zeroscope_v2_576w\",\n",
    "    device=\"cuda\",\n",
    "    num_inference_steps=50,\n",
    "    chunk_size=8,\n",
    "    overlap=2,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models\n",
    "\n",
    "Load all required model components (VAE, text encoder, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 15:16:01,382 - distributed_coordinator - INFO - Loading pipeline components from cerspense/zeroscope_v2_576w\n",
      "Fetching 12 files: 100%|██████████| 12/12 [01:17<00:00,  6.49s/it]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:34<00:00,  6.91s/it]\n"
     ]
    }
   ],
   "source": [
    "coordinator.load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Latents\n",
    "\n",
    "Create initial noise latents and encode the text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: torch.Size([1, 4, 16, 40, 72])\n",
      "Text embeddings shape: torch.Size([1, 77, 1024])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A rocket launching into space, cinematic, detailed, 4K\"\n",
    "num_frames = 16\n",
    "\n",
    "latents, text_embeddings = coordinator.prepare_latents(prompt, num_frames)\n",
    "print(f\"Latents shape: {latents.shape}\")\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split into Chunks\n",
    "\n",
    "Split the latent tensor into overlapping chunks for distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3\n",
      "Chunk 0: frames 0-8, shape torch.Size([1, 4, 8, 40, 72])\n",
      "Chunk 1: frames 6-14, shape torch.Size([1, 4, 8, 40, 72])\n",
      "Chunk 2: frames 12-16, shape torch.Size([1, 4, 4, 40, 72])\n"
     ]
    }
   ],
   "source": [
    "chunks = coordinator.split_into_chunks(latents, text_embeddings)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: frames {chunk['start_idx']}-{chunk['end_idx']}, shape {chunk['chunk'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Chunks\n",
    "\n",
    "Save each chunk to disk for processing by workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 0 to ./output/chunks/chunk_0.pkl\n",
      "Saved chunk 1 to ./output/chunks/chunk_1.pkl\n",
      "Saved chunk 2 to ./output/chunks/chunk_2.pkl\n"
     ]
    }
   ],
   "source": [
    "chunk_files = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_file = coordinator.save_chunk(chunk, i)\n",
    "    chunk_files.append(chunk_file)\n",
    "    print(f\"Saved chunk {i} to {chunk_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Performance Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "def log_vram_usage(stage=\"\"):\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0 by default\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    used = info.used // 1024**2\n",
    "    total = info.total // 1024**2\n",
    "    print(f\"[{stage}] GPU VRAM Usage: {used} MB / {total} MB\")\n",
    "    pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Launch Workers\n",
    "\n",
    "Launch worker processes to process each chunk. In a real distributed setting, these would run on separate machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = []\n",
    "for i, chunk_file in enumerate(chunk_files):\n",
    "    output_file = coordinator.launch_worker(chunk_file, i)\n",
    "    output_files.append(output_file)\n",
    "    print(f\"Launched worker for chunk {i}, output will be saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wait for Workers\n",
    "\n",
    "Wait for all workers to complete processing. In a real distributed setting, you might want to implement a more sophisticated waiting mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    all_done = True\n",
    "    for output_file in output_files:\n",
    "        if not os.path.exists(output_file):\n",
    "            all_done = False\n",
    "            break\n",
    "    \n",
    "    if all_done:\n",
    "        print(\"All workers have completed!\")\n",
    "        break\n",
    "    \n",
    "    print(\"Waiting for workers to complete...\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Results\n",
    "\n",
    "Load the processed chunks from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks = coordinator.load_results(output_files)\n",
    "print(f\"Loaded {len(processed_chunks)} processed chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stitch Chunks\n",
    "\n",
    "Stitch the processed chunks back together, averaging overlapping regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_latents = coordinator.stitch_chunks(processed_chunks, latents.shape)\n",
    "print(f\"Stitched latents shape: {stitched_latents.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Decode to Video\n",
    "\n",
    "Decode the stitched latents into a video and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"final_video.mp4\"\n",
    "coordinator.decode_to_video(stitched_latents, output_path)\n",
    "print(f\"Video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup\n",
    "\n",
    "Clean up temporary files (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bt-subnet)",
   "language": "python",
   "name": "bt-subnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
